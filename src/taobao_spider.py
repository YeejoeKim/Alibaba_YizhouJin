import time
import pandas as pd
import re
import random
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options

# ================= é…ç½®åŒº =================
# 20ä¸ªä¸»æµç²¾é€‰ç±»ç›®
KEYWORDS = [
    # ç¬¬ä¸€ç»„ï¼šæœé¥°ç©¿æ­
    "Tæ¤", "å«è¡£", "ç‰›ä»”è£¤", "è¿åŠ¨é‹",
    # ç¬¬äºŒç»„ï¼šç¾å¦†ä¸ªæŠ¤
    "å£çº¢", "é¢è†œ", "æ´—å‘æ°´", "é¦™æ°´",
    # ç¬¬ä¸‰ç»„ï¼šå®¶å±…æ—¥ç”¨
    "å››ä»¶å¥—", "ä¿æ¸©æ¯", "æ”¶çº³ç®±", "æŠ½çº¸",
    # ç¬¬å››ç»„ï¼šæ•°ç 3C
    "æ‰‹æœºå£³", "è“ç‰™è€³æœº", "å……ç”µå®", "æ™ºèƒ½æ‰‹ç¯",
    # ç¬¬äº”ç»„ï¼šç”Ÿæ´»å…´è¶£
    "çŒ«ç²®", "ç‘œä¼½å«", "åšæœ", "éœ²è¥ç¯"
]

# æ¯ä¸ªç±»ç›®æŠ“ 20 ä¸ªï¼Œé¢„è®¡æ€»å…±è·å– 400 æ¡æ•°æ®
MAX_ITEMS_PER_CAT = 20


# =========================================

def get_taobao_data():
    print(">>> å¯åŠ¨æµè§ˆå™¨...")

    # 1. è®¾ç½®æµè§ˆå™¨é˜²æ£€æµ‹å‚æ•°
    options = Options()
    # ã€å…³é”®ã€‘è„šæœ¬ç»“æŸåä¿æŒæµè§ˆå™¨å¼€å¯
    options.add_experimental_option("detach", True)
    # å»é™¤è‡ªåŠ¨åŒ–æ§åˆ¶ç‰¹å¾
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])

    driver = webdriver.Chrome(options=options)

    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶åï¼Œé˜²æ­¢æ–‡ä»¶è¢«å ç”¨æŠ¥é”™
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"taobao_data_{timestamp}.csv"

    try:
        # 2. æ‰«ç ç™»å½•ç¯èŠ‚
        print(">>> æ‰“å¼€ç™»å½•é¡µ...")
        driver.get("https://login.taobao.com/member/login.jhtml")
        print("\nâš ï¸âš ï¸âš ï¸ è¯·æ‹¿å‡ºæ‰‹æœºæ‰«ç ç™»å½•ï¼(ç¨‹åºå°†ä¸€ç›´ç­‰å¾…ï¼Œç›´åˆ°ä½ å®Œæˆè·³è½¬) âš ï¸âš ï¸âš ï¸")

        # å¾ªç¯æ£€æµ‹æ˜¯å¦è·³è½¬
        while "login.taobao.com" in driver.current_url:
            time.sleep(1)
        print(">>> æ£€æµ‹åˆ°ç™»å½•æˆåŠŸï¼")

        all_data = []

        # 3. å¾ªç¯æŠ“å– 20 ä¸ªç±»ç›®
        for keyword in KEYWORDS:
            print(f"\n>>> [{keyword}] æ­£åœ¨æœç´¢...")
            try:
                driver.get(f"https://s.taobao.com/search?q={keyword}")

                # ==========================================
                # ğŸ›‘ äººå·¥ç¡®è®¤ç¯èŠ‚ (é˜²æ­¢ç™½å±/éªŒè¯ç )
                # ==========================================
                print("\n" + "=" * 50)
                print(f"å½“å‰è¿›åº¦: æ­£åœ¨æŠ“å– [{keyword}]")
                print("1. è¯·åˆ‡æ¢å›æµè§ˆå™¨ï¼Œçœ‹ä¸€çœ¼å•†å“åˆ—è¡¨æ˜¯å¦åŠ è½½å‡ºæ¥ï¼Ÿ")
                print("2. å¦‚æœæ˜¯ç™½å±ï¼Œè¯·æ‰‹åŠ¨ã€åˆ·æ–°ã€‘æˆ–ã€ä¸‹æ»‘ã€‘ã€‚")
                print("3. ç¡®è®¤çœ‹åˆ°å•†å“åï¼Œå›æ¥è¿™é‡Œè¾“å…¥ 'y' å¹¶æŒ‰å›è½¦...")
                print("=" * 50)

                while True:
                    # åªè¦ä¸è¾“å…¥yï¼Œå°±æ­»å¾ªç¯ç­‰å¾…ï¼Œç¡®ä¿ä½ å‡†å¤‡å¥½äº†
                    if input(">>> å•†å“å‡ºæ¥äº†å—ï¼Ÿ(è¾“å…¥ y ç»§ç»­): ").lower() == 'y':
                        break

                print(f">>> æ­£åœ¨æå– [{keyword}] çš„æ•°æ®...")

                # æŸ¥æ‰¾é¡µé¢æ‰€æœ‰é“¾æ¥
                links = driver.find_elements(By.TAG_NAME, "a")

                count = 0
                seen = set()  # å»é‡é›†åˆ

                for link in links:
                    if count >= MAX_ITEMS_PER_CAT: break

                    try:
                        href = link.get_attribute("href")
                        raw_text = link.text

                        # ç­›é€‰é€»è¾‘ï¼šå¿…é¡»æ˜¯å•†å“é“¾æ¥ï¼Œä¸”æ–‡æœ¬å†…å®¹ä¸°å¯Œ
                        if href and "item.htm" in href and len(raw_text) > 10 and "click" not in href:
                            if href in seen: continue
                            seen.add(href)

                            # --- A. æå–å›¾ç‰‡ (å«æ‡’åŠ è½½å¤„ç†) ---
                            img_url = ""
                            try:
                                img_elem = link.find_element(By.TAG_NAME, "img")
                                img_url = img_elem.get_attribute("src")
                                if not img_url or "base64" in img_url or "blank" in img_url:
                                    lazy_src = img_elem.get_attribute("data-src")
                                    if lazy_src: img_url = lazy_src
                                if img_url and img_url.startswith("//"):
                                    img_url = "https:" + img_url
                            except:
                                pass

                            # --- B. æå–ä»·æ ¼ ---
                            price = "0"
                            price_match = re.search(r'[Â¥ï¿¥]\s*(\d+(\.\d+)?)', raw_text)
                            if price_match: price = price_match.group(1)

                            # --- C. å¤šç»´çƒ­åº¦æå– (æ™ºèƒ½è¯†åˆ«é”€é‡/å›å¤´å®¢/è¯„ä»·) ---
                            sales = "0"
                            sales_type = "æ— æ•°æ®"

                            # C1. ä¼˜å…ˆæ‰¾ "ä»˜æ¬¾/å·²å”®"
                            pay_match = re.search(r'(\d+(?:\.\d+)?[ä¸‡wW]?\+?)\s*(äººä»˜æ¬¾|å·²å”®|æœˆé”€|ä»˜æ¬¾)', raw_text)
                            if pay_match:
                                sales = pay_match.group(1)
                                sales_type = "ä»˜æ¬¾äººæ•°"
                            else:
                                # C2. æ²¡é”€é‡ï¼Ÿæ‰¾ "å›å¤´å®¢"
                                loyal_match = re.search(r'å›å¤´å®¢\s*(\d+(?:\.\d+)?[ä¸‡wW]?\+?)', raw_text)
                                if loyal_match:
                                    sales = loyal_match.group(1)
                                    sales_type = "å›å¤´å®¢æ•°"
                                else:
                                    # C3. ä¹Ÿæ²¡å›å¤´å®¢ï¼Ÿæ‰¾ "è¯„ä»·"
                                    comment_match = re.search(r'(\d+(?:\.\d+)?[ä¸‡wW]?\+?)\s*(æ¡?è¯„ä»·|æ¡?è¯„è®º)',
                                                              raw_text)
                                    if comment_match:
                                        sales = comment_match.group(1)
                                        sales_type = "è¯„ä»·æ•°"

                            # --- D. æå–æ ‡é¢˜ ---
                            lines = raw_text.split('\n')
                            title = max(lines, key=len) if lines else raw_text

                            # æ§åˆ¶å°æ‰“å°ä¸€æ¡é¢„è§ˆ
                            print(f"   âœ… {title[:10]}... | ğŸ’°{price} | ğŸ”¥{sales}({sales_type})")

                            all_data.append({
                                "ç±»ç›®": keyword,
                                "æ ‡é¢˜": title,
                                "ä»·æ ¼": price,
                                "çƒ­åº¦æ•°å€¼": sales,
                                "çƒ­åº¦ç±»å‹": sales_type,
                                "ä¸»å›¾é“¾æ¥": img_url,
                                "å•†å“é“¾æ¥": href,
                                "åŸå§‹æ–‡æœ¬": raw_text.replace('\n', ' ')
                            })
                            count += 1

                    except:
                        continue

            except Exception as e:
                print(f"âŒ æŠ“å–ç±»ç›® [{keyword}] æ—¶å‡ºé”™: {e}")

            # éšæœºä¼‘æ¯ 3-6 ç§’ï¼Œæ¨¡æ‹ŸçœŸäººæ“ä½œï¼Œé˜²æ­¢ç¿»é¡µå¤ªå¿«è¢«å°
            sleep_time = random.uniform(3, 6)
            print(f">>> æœ¬ç±»ç›®å®Œæˆï¼Œä¼‘æ¯ {sleep_time:.1f} ç§’...")
            time.sleep(sleep_time)

        # 4. ä¿å­˜æœ€ç»ˆæ–‡ä»¶
        if all_data:
            df = pd.DataFrame(all_data)
            df.to_csv(output_file, index=False, encoding="utf_8_sig")
            print(f"\nğŸ‰ğŸ‰ğŸ‰ å…¨éƒ¨ä»»åŠ¡å®Œæˆï¼")
            print(f"ğŸ‘‰ å…±æŠ“å– {len(all_data)} æ¡æ•°æ®")
            print(f"ğŸ‘‰ æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            print(f"ğŸ‘‰ (è¯·ç”¨ Excel æ‰“å¼€æ£€æŸ¥ï¼Œè‹¥æœ‰ä¹±ç è¯·ç”¨è®°äº‹æœ¬æ‰“å¼€å†å¦å­˜ä¸ºANSI)")
        else:
            print("\nğŸ˜­ æœ¬æ¬¡æœªæŠ“å–åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ç™»å½•çŠ¶æ€ã€‚")

    except Exception as e:
        print(f"âŒ ç¨‹åºå‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

    finally:
        print("\nğŸ›‘ ç¨‹åºç»“æŸï¼Œæµè§ˆå™¨å°†ä¿æŒæ‰“å¼€çŠ¶æ€ã€‚")


if __name__ == "__main__":
    get_taobao_data()